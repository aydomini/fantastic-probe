#!/bin/bash
export LC_ALL=C.UTF-8

#==============================================================================
# ISO Media Info Extraction Service - Cron Scanner Mode
# Scans for unprocessed files every minute (alternative to inotifywait)
# Author: Fantastic-Probe Team
#==============================================================================

set -euo pipefail

# Read version dynamically
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VERSION="1.2.2"  # Hardcoded default

if [ -f "$SCRIPT_DIR/get-version.sh" ]; then
    source "$SCRIPT_DIR/get-version.sh"
elif command -v git &> /dev/null && [ -d "$SCRIPT_DIR/.git" ]; then
    VERSION=$(git -C "$SCRIPT_DIR" describe --tags --abbrev=0 2>/dev/null | sed 's/^v//' || echo "4.2.2")
fi

#==============================================================================
# Configuration
#==============================================================================

# Configuration file path
CONFIG_FILE="${CONFIG_FILE:-/etc/fantastic-probe/config}"

# Default configuration
STRM_ROOT="/mnt/sata1/media/åª’ä½“åº“/strm"
LOG_FILE="/var/log/fantastic_probe.log"
ERROR_LOG_FILE="/var/log/fantastic_probe_errors.log"

# Cron-specific configuration
CRON_LOCK_FILE="/tmp/fantastic_probe_cron_scanner.lock"
FAILURE_CACHE_DB="/var/lib/fantastic-probe/failure_cache.db"
MAX_RETRY_COUNT=3  # Stop retrying after this many failures
SCAN_BATCH_SIZE=10  # Max files to process per scan

# Load configuration file
if [ -f "$CONFIG_FILE" ]; then
    # shellcheck source=/dev/null
    source "$CONFIG_FILE"
fi

# Compatibility with config file variable names (config.template uses CRON_ prefix)
MAX_RETRY_COUNT=${CRON_MAX_RETRY_COUNT:-$MAX_RETRY_COUNT}
SCAN_BATCH_SIZE=${CRON_SCAN_BATCH_SIZE:-$SCAN_BATCH_SIZE}

# Ensure failure cache directory exists
CACHE_DIR=$(dirname "$FAILURE_CACHE_DB")
mkdir -p "$CACHE_DIR"

#==============================================================================
# Logging functions
#==============================================================================

log() {
    local timestamp
    timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    # Log only to file to avoid duplicate output (crontab captures stderr)
    echo "[$timestamp] [CRON] $1" >> "$LOG_FILE"
}

log_info() {
    log "â„¹ï¸  INFO: $1"
}

log_warn() {
    log "âš ï¸  WARN: $1"
}

log_error() {
    log "âŒ ERROR: $1"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [CRON] $1" >> "$ERROR_LOG_FILE"
}

log_success() {
    log "âœ… SUCCESS: $1"
}

log_debug() {
    if [ "${DEBUG:-false}" = "true" ]; then
        log "ğŸ” DEBUG: $1"
    fi
}

#==============================================================================
# Concurrency control (flock mechanism)
#==============================================================================

acquire_lock() {
    # Try to acquire lock (non-blocking)
    exec 200>"$CRON_LOCK_FILE"

    if ! flock -n 200; then
        log_warn "ä¸Šä¸€ä¸ªæ‰«æä»»åŠ¡ä»åœ¨è¿è¡Œï¼Œè·³è¿‡æœ¬æ¬¡æ‰«æ"
        return 1
    fi

    # Write current PID
    echo $$ >&200
    log_debug "å·²è·å–æ‰«æé”ï¼ˆPID: $$ï¼‰"
    return 0
}

release_lock() {
    # Lock is automatically released on script exit (file descriptor closed)
    log_debug "é‡Šæ”¾æ‰«æé”"
}

trap release_lock EXIT

#==============================================================================
# Cleanup stale mount points (prevent leftover from previous abnormal exits)
#==============================================================================

cleanup_stale_mounts() {
    log_debug "æ£€æŸ¥å¹¶æ¸…ç†æ®‹ç•™çš„ bd-lang æŒ‚è½½ç‚¹..."

    # Find all /tmp/bd-lang-* mount points (extract path between "on" and "type")
    local stale_mounts=$(mount | grep "/tmp/bd-lang-" | sed -E 's/.* on (\/tmp\/bd-lang-[0-9]+) type .*/\1/' || true)

    if [ -n "$stale_mounts" ]; then
        log_warn "å‘ç°æ®‹ç•™æŒ‚è½½ç‚¹ï¼Œæ­£åœ¨æ¸…ç†..."
        echo "$stale_mounts" | while read -r mount_point; do
            # Validate mount point path is not empty and has correct format
            if [ -n "$mount_point" ] && [[ "$mount_point" =~ ^/tmp/bd-lang-[0-9]+$ ]]; then
                log_info "  æ¸…ç†æŒ‚è½½ç‚¹: $mount_point"
                sudo umount -f "$mount_point" 2>/dev/null || true
                sudo rmdir "$mount_point" 2>/dev/null || true
            else
                log_debug "  è·³è¿‡æ— æ•ˆè·¯å¾„: '$mount_point'"
            fi
        done
    fi

    # Clean empty /tmp/bd-lang-* directories
    find /tmp -maxdepth 1 -type d -name "bd-lang-*" -empty -exec sudo rmdir {} \; 2>/dev/null || true
}

#==============================================================================
# Failure cache management (SQLite)
#==============================================================================

init_failure_cache() {
    # Initialize SQLite database
    sqlite3 "$FAILURE_CACHE_DB" <<'SQL'
CREATE TABLE IF NOT EXISTS failure_cache (
    file_path TEXT PRIMARY KEY,
    failure_count INTEGER DEFAULT 0,
    last_failure_time INTEGER,
    last_error_message TEXT,
    created_at INTEGER DEFAULT (strftime('%s', 'now'))
);

CREATE INDEX IF NOT EXISTS idx_failure_count ON failure_cache(failure_count);
CREATE INDEX IF NOT EXISTS idx_last_failure_time ON failure_cache(last_failure_time);
SQL

    log_debug "å¤±è´¥ç¼“å­˜æ•°æ®åº“å·²åˆå§‹åŒ–"
}

should_skip_file() {
    local file_path="$1"

    # Query failure count
    local failure_count
    failure_count=$(sqlite3 "$FAILURE_CACHE_DB" \
        "SELECT failure_count FROM failure_cache WHERE file_path='$file_path';" 2>/dev/null || echo "0")

    if [ -z "$failure_count" ]; then
        failure_count=0
    fi

    # Check if exceeds max retry count
    if [ "$failure_count" -ge "$MAX_RETRY_COUNT" ]; then
        log_debug "è·³è¿‡ï¼ˆå·²å¤±è´¥ $failure_count æ¬¡ï¼‰: $file_path"
        return 0  # Skip
    fi

    return 1  # Don't skip
}

record_failure() {
    local file_path="$1"
    local error_message="${2:-æœªçŸ¥é”™è¯¯}"
    local current_time
    current_time=$(date +%s)

    # Insert or update failure record
    sqlite3 "$FAILURE_CACHE_DB" <<SQL
INSERT INTO failure_cache (file_path, failure_count, last_failure_time, last_error_message)
VALUES ('$file_path', 1, $current_time, '$error_message')
ON CONFLICT(file_path) DO UPDATE SET
    failure_count = failure_count + 1,
    last_failure_time = $current_time,
    last_error_message = '$error_message';
SQL

    # Get updated failure count
    local new_count
    new_count=$(sqlite3 "$FAILURE_CACHE_DB" \
        "SELECT failure_count FROM failure_cache WHERE file_path='$file_path';")

    log_warn "æ–‡ä»¶å¤„ç†å¤±è´¥ï¼ˆç¬¬ $new_count/$MAX_RETRY_COUNT æ¬¡ï¼‰: $(basename "$file_path")"

    if [ "$new_count" -ge "$MAX_RETRY_COUNT" ]; then
        log_error "æ–‡ä»¶å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°†ä¸å†å°è¯•: $file_path"
        log_error "é”™è¯¯åŸå› : $error_message"
        log_info "å¦‚éœ€é‡æ–°å°è¯•ï¼Œè¯·åˆ é™¤ç¼“å­˜æ•°æ®åº“: $FAILURE_CACHE_DB"
    fi
}

clear_failure_cache() {
    # Clear failure cache (called on restart)
    if [ -f "$FAILURE_CACHE_DB" ]; then
        rm -f "$FAILURE_CACHE_DB"
        log_info "å¤±è´¥ç¼“å­˜å·²æ¸…ç©º"
    fi
}

get_failure_stats() {
    # Get failure statistics
    if [ ! -f "$FAILURE_CACHE_DB" ]; then
        echo "å¤±è´¥ç¼“å­˜æ•°æ®åº“ä¸å­˜åœ¨"
        return
    fi

    local total_failures
    local permanent_failures

    total_failures=$(sqlite3 "$FAILURE_CACHE_DB" "SELECT COUNT(*) FROM failure_cache;" 2>/dev/null || echo "0")
    permanent_failures=$(sqlite3 "$FAILURE_CACHE_DB" "SELECT COUNT(*) FROM failure_cache WHERE failure_count >= $MAX_RETRY_COUNT;" 2>/dev/null || echo "0")

    echo "å¤±è´¥ç¼“å­˜ç»Ÿè®¡: æ€»è®¡ $total_failures ä¸ªæ–‡ä»¶ï¼Œæ°¸ä¹…å¤±è´¥ $permanent_failures ä¸ª"
}

#==============================================================================
# Process single file (using standalone process library)
#==============================================================================

# Load process library functions
load_process_library() {
    local lib_paths=(
        "/usr/local/lib/fantastic-probe-process-lib.sh"
        "$SCRIPT_DIR/fantastic-probe-process-lib.sh"
        "/usr/local/bin/fantastic-probe-process-lib.sh"
    )

    for lib_path in "${lib_paths[@]}"; do
        if [ -f "$lib_path" ]; then
            log_debug "åŠ è½½å¤„ç†åº“: $lib_path"
            # shellcheck source=/dev/null
            source "$lib_path"
            return 0
        fi
    done

    log_error "æ‰¾ä¸åˆ°å¤„ç†åº“æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„ï¼š"
    for lib_path in "${lib_paths[@]}"; do
        log_error "  - $lib_path"
    done
    return 1
}

process_iso_strm() {
    local strm_file="$1"

    # Check failure cache
    if should_skip_file "$strm_file"; then
        return 0
    fi

    log_info "å¼€å§‹å¤„ç†: $(basename "$strm_file")"

    # Call function from process library
    local error_output
    local exit_code

    set +e
    error_output=$(process_iso_strm_full "$strm_file" 2>&1)
    exit_code=$?
    set -e

    if [ $exit_code -eq 0 ]; then
        log_success "å¤„ç†æˆåŠŸ: $(basename "$strm_file")"
        return 0
    else
        # Extract error message (last line)
        local error_message
        error_message=$(echo "$error_output" | tail -1 | sed 's/.*ERROR: //' || echo "å¤„ç†å¤±è´¥")

        log_error "å¤„ç†å¤±è´¥: $(basename "$strm_file") - $error_message"
        record_failure "$strm_file" "$error_message"
        return 1
    fi
}

#==============================================================================
# Scan for unprocessed files
#==============================================================================

scan_and_process() {
    # Validate monitoring directory
    if [ ! -d "$STRM_ROOT" ]; then
        log_error "STRM æ ¹ç›®å½•ä¸å­˜åœ¨: $STRM_ROOT"
        return 1
    fi

    # Initialize failure cache (silent)
    init_failure_cache

    # Find all .iso.strm files without JSON
    local pending_files=()

    while IFS= read -r -d '' strm_file; do
        local strm_dir
        local strm_name
        local json_file

        strm_dir="$(dirname "$strm_file")"
        strm_name="$(basename "$strm_file" .iso.strm)"
        json_file="${strm_dir}/${strm_name}.iso-mediainfo.json"

        # Check if JSON already exists
        if [ ! -f "$json_file" ]; then
            pending_files+=("$strm_file")
        fi
    done < <(find "$STRM_ROOT" -type f -name "*.iso.strm" -print0 2>/dev/null)

    local total_pending=${#pending_files[@]}

    # Completely silent on empty scans
    if [ $total_pending -eq 0 ]; then
        return 0
    fi

    # Batch processing (limit per scan to avoid long running)
    local processed=0
    local succeeded=0
    local failed=0

    for strm_file in "${pending_files[@]}"; do
        # Stop at batch limit
        if [ $processed -ge $SCAN_BATCH_SIZE ]; then
            log_warn "å·²è¾¾åˆ°æ‰¹é‡é™åˆ¶ï¼ˆ$SCAN_BATCH_SIZEï¼‰ï¼Œå‰©ä½™ $((total_pending - processed)) ä¸ªæ–‡ä»¶å°†åœ¨ä¸‹æ¬¡æ‰«æå¤„ç†"
            break
        fi

        # Process file (serial to prevent resource exhaustion)
        if process_iso_strm "$strm_file"; then
            ((succeeded++)) || true
        else
            ((failed++)) || true
        fi

        ((processed++)) || true

        # Interval between tasks (prevent cloud storage rate limiting)
        if [ $processed -lt $SCAN_BATCH_SIZE ] && [ $processed -lt $total_pending ]; then
            sleep 10
        fi
    done

    return 0
}

#==============================================================================
# Main function
#==============================================================================

main() {
    # Check if SQLite is installed
    if ! command -v sqlite3 &> /dev/null; then
        log_error "æœªå®‰è£… sqlite3ï¼Œè¯·æ‰§è¡Œ: apt-get install sqlite3"
        exit 1
    fi

    # Load process library
    if ! load_process_library; then
        log_error "åŠ è½½å¤„ç†åº“å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œ"
        exit 1
    fi

    # Check dependencies (silent check on startup)
    if ! check_dependencies; then
        log_error "ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…ç¼ºå¤±çš„ä¾èµ–åé‡è¯•"
        log_error "è¯¦ç»†ä¿¡æ¯è§ä¸Šæ–¹æ—¥å¿—"
        exit 1
    fi
    # No output when dependencies are satisfied to keep logs clean

    # Try to acquire lock
    if ! acquire_lock; then
        exit 0  # Silent exit (previous task still running)
    fi

    # Clean up stale mount points
    cleanup_stale_mounts

    # Execute scan
    scan_and_process

    # Lock is automatically released in EXIT trap
}

# Support command line arguments
case "${1:-scan}" in
    scan)
        main
        ;;
    clear-cache)
        log_info "æ¸…ç©ºå¤±è´¥ç¼“å­˜..."
        clear_failure_cache
        log_success "å¤±è´¥ç¼“å­˜å·²æ¸…ç©º"
        ;;
    stats)
        init_failure_cache
        get_failure_stats

        # Show detailed information
        if [ -f "$FAILURE_CACHE_DB" ]; then
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo "æ°¸ä¹…å¤±è´¥çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆå¤±è´¥æ¬¡æ•° >= $MAX_RETRY_COUNTï¼‰ï¼š"
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            echo ""

            # Check if there are failed files
            failure_count=$(sqlite3 "$FAILURE_CACHE_DB" \
                "SELECT COUNT(*) FROM failure_cache WHERE failure_count >= $MAX_RETRY_COUNT;" 2>/dev/null || echo "0")

            if [ "$failure_count" -eq 0 ]; then
                echo "  âœ… æš‚æ— æ°¸ä¹…å¤±è´¥çš„æ–‡ä»¶"
            else
                # Use formatted output (table mode)
                sqlite3 -header -column "$FAILURE_CACHE_DB" <<SQL
.width 50 8 20 40
SELECT
    file_path AS 'æ–‡ä»¶è·¯å¾„',
    failure_count AS 'å¤±è´¥æ¬¡æ•°',
    datetime(last_failure_time, 'unixepoch', 'localtime') AS 'æœ€åå¤±è´¥æ—¶é—´',
    CASE
        WHEN length(last_error_message) > 40
        THEN substr(last_error_message, 1, 37) || '...'
        ELSE last_error_message
    END AS 'é”™è¯¯ä¿¡æ¯'
FROM failure_cache
WHERE failure_count >= $MAX_RETRY_COUNT
ORDER BY last_failure_time DESC;
SQL
            fi
            echo ""
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
        fi
        ;;
    reset-file)
        if [ -z "${2:-}" ]; then
            echo "ç”¨æ³•: $0 reset-file <æ–‡ä»¶è·¯å¾„>"
            exit 1
        fi

        init_failure_cache
        sqlite3 "$FAILURE_CACHE_DB" "DELETE FROM failure_cache WHERE file_path='$2';"
        log_success "å·²é‡ç½®æ–‡ä»¶çš„å¤±è´¥è®°å½•: $2"
        ;;
    *)
        echo "ç”¨æ³•: $0 {scan|clear-cache|stats|reset-file <æ–‡ä»¶è·¯å¾„>}"
        echo ""
        echo "å‘½ä»¤è¯´æ˜ï¼š"
        echo "  scan         æ‰§è¡Œæ‰«æå’Œå¤„ç†ï¼ˆé»˜è®¤ï¼‰"
        echo "  clear-cache  æ¸…ç©ºå¤±è´¥ç¼“å­˜æ•°æ®åº“"
        echo "  stats        æ˜¾ç¤ºå¤±è´¥ç»Ÿè®¡ä¿¡æ¯"
        echo "  reset-file   é‡ç½®æŒ‡å®šæ–‡ä»¶çš„å¤±è´¥è®°å½•"
        exit 1
        ;;
esac
